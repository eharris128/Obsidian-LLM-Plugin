import { ImageSize, ViewType } from "Types/types";
import LLMPlugin from "main";
import { DropdownComponent, Setting } from "obsidian";
import { Assistant } from "openai/resources/beta/assistants";
import { modelNames, models } from "utils/models";
import {
	getAssistant,
	getSettingType,
	getViewInfo,
	getGpt4AllPath
} from "utils/utils";
import { assistant as ASSISTANT, chat, GPT4All, messages, openAI } from "utils/constants"
import { Header } from "./Header";
import { FileSelector } from "./FileSelector";

export class SettingsContainer {
	viewType: ViewType;

	constructor(private plugin: LLMPlugin, viewType: ViewType) {
		this.viewType = viewType;
	}

	async generateSettingsContainer(parentContainer: HTMLElement, Header: Header) {
		this.resetSettings(parentContainer);
		this.generateModels(parentContainer, Header);
		this.generateModelSettings(parentContainer);
	}

	generateModels(parentContainer: HTMLElement, Header: Header) {
		const settingType = getSettingType(this.viewType);
		const viewSettings = this.plugin.settings[settingType];

		new Setting(parentContainer)
			.setName("Models")
			.setDesc("The model you want to use to generate a chat response.")
			.addDropdown((dropdown: DropdownComponent) => {
				dropdown.addOption("", "---Select assistant---");
				const assistants = this.plugin.settings.assistants;
				assistants.map((assistant: Assistant) => {
					dropdown.addOption(`${assistant.id}`, `${assistant.name}`);
				});
				dropdown.addOption("", "---Select model---");
				let keys = Object.keys(models);
				for (let model of keys) {
					if (models[model].type === GPT4All) {
						const gpt4AllPath = getGpt4AllPath(this.plugin);
						const fullPath = `${gpt4AllPath}/${models[model].model}`;
						const exists = this.plugin.fileSystem.existsSync(fullPath);
						if (exists) {
							dropdown.addOption(models[model].model, model);
						}
					} else {
						dropdown.addOption(models[model].model, model);
					}
				}
				dropdown.onChange((change) => {
					const { historyIndex } = getViewInfo(
						this.plugin,
						this.viewType
					);
					const index = historyIndex;
					if (change.includes("asst")) {
						viewSettings.assistant = true;
						this.plugin.saveSettings();
					} else {
						viewSettings.assistant = false;
						viewSettings.assistantId = "";
						this.plugin.saveSettings();
					}
					if (!viewSettings.assistant) {
						const modelName = modelNames[change];
						viewSettings.model = change;
						viewSettings.modelName = modelName;
						viewSettings.modelType = models[modelName].type;
						viewSettings.endpointURL = models[modelName].url;
						viewSettings.modelEndpoint = models[modelName].endpoint;
						if (index > -1) {
							this.plugin.settings.promptHistory[index].model =
								change;
							this.plugin.settings.promptHistory[
								index
							].modelName = modelName;
						}
						this.plugin.saveSettings();
						Header.setHeader(modelName);
					}
					if (viewSettings.assistant) {
						viewSettings.assistantId = change;
						const assistant = getAssistant(
							this.plugin,
							viewSettings.assistantId
						);
						viewSettings.model = assistant!.id;
						viewSettings.modelName = assistant!.name as string;
						viewSettings.modelType = assistant!.modelType;
						viewSettings.endpointURL = "";
						viewSettings.modelEndpoint = ASSISTANT;
						if (index > -1) {
							this.plugin.settings.promptHistory[index].model =
								assistant!.model;
							this.plugin.settings.promptHistory[
								index
							].modelName = modelNames[assistant!.model];
						}
						this.plugin.saveSettings();
						Header.setHeader(assistant.name as string);
					}
					this.generateSettingsContainer(parentContainer, Header);
				});
				dropdown.setValue(viewSettings.model);
			});
	}

	resetSettings(parentContainer: Element) {
		parentContainer.empty();
	}

	generateModelSettings(parentContainer: HTMLElement) {
		const settingType = getSettingType(this.viewType);
		const viewSettings = this.plugin.settings[settingType];
		const endpoint = viewSettings.modelEndpoint;
		const modelType = viewSettings.modelType;
		if (endpoint === "images") {
			this.generateImageSettings(parentContainer, viewSettings.model);
		}
		if (endpoint === "moderations") {
			this.generateModerationsSettings(parentContainer);
		}
		if (endpoint === chat || messages) {
			this.generateChatSettings(parentContainer, modelType);
			this.generateContextSettings(parentContainer);
		}
	}

	generateImageSettings(parentContainer: HTMLElement, model: string) {
		const settingType = getSettingType(this.viewType);
		const viewSettings = this.plugin.settings[settingType];
		const imageSizes = {
			dallE2: ["256x256", "512x512", "1024x1024"],
			dallE3: ["1024x1024", "1792x1024", "1024x1792"],
		};
		new Setting(parentContainer)
			.setName("Number of images")
			.setDesc(
				"The number of images generated by the model. Must be between 1 and 10. For Dall-E 3, only 1 image can be generated."
			)
			.addText((text) => {
				text.setValue(`${viewSettings.imageSettings.numberOfImages}`);
				text.inputEl.type = "number";
				text.onChange((change) => {
					viewSettings.imageSettings.numberOfImages =
						parseInt(change);
					this.plugin.saveSettings();
				});
			});

		new Setting(parentContainer)
			.setName("Response format")
			.setDesc(
				"The format in which the generated images are returned. Must be one of url or b64_json. URLs are only valid for 60 minutes after the image has been generated."
			)
			.addDropdown((dropdown: DropdownComponent) => {
				dropdown.addOption("", "Select a Format");
				dropdown.addOption("url", "URL");
				dropdown.addOption("b64_json", "Base64 JSON");
				dropdown.onChange((change) => {
					viewSettings.imageSettings.response_format = change as
						| "url"
						| "b64_json";
					this.plugin.saveSettings();
				});
			});

		new Setting(parentContainer)
			.setName("Image size")
			.setDesc(
				"The size of the generated images. Must be one of 256x256, 512x512, or 1024x1024 for dall-e-2. Must be one of 1024x1024, 1792x1024, or 1024x1792 for dall-e-3 models."
			)
			.addDropdown((dropdown: DropdownComponent) => {
				if (model === "dall-e-2") {
					dropdown.addOption("", "Dall-E 2 sizes");
					imageSizes["dallE2"].map((size) => {
						dropdown.addOption(size, size);
					});
				}
				if (model === "dall-e-3") {
					dropdown.addOption("", "Dall-E 3 sizes");
					imageSizes["dallE3"].map((size) => {
						dropdown.addOption(size, size);
					});
				}

				dropdown.onChange((change: ImageSize) => {
					viewSettings.imageSettings.size = change;
					this.plugin.saveSettings();
				});
			});

		if (model === "dall-e-3") {
			new Setting(parentContainer)
				.setName("Image style")
				.setDesc(
					"Defaults to vivid. Must be one of vivid or natural. Vivid causes the model to lean towards generating hyper-real and dramatic images. Natural causes the model to produce more natural, less hyper-real looking images. This param is only supported for dall-e-3."
				)
				.addDropdown((dropdown: DropdownComponent) => {
					dropdown.addOption("", "Select style");
					dropdown.addOption("natural", "Natural");
					dropdown.addOption("vivid", "Vivid");
					dropdown.onChange((change: "vivid" | "natural") => {
						viewSettings.imageSettings.style = change;
						this.plugin.saveSettings();
					});
				});

			new Setting(parentContainer)
				.setName("Quality")
				.setDesc(
					"The quality of the image that will be generated. hd creates images with finer details and greater consistency across the image. This param is only supported for dall-e-3."
				)
				.addToggle((value) => {
					value.onChange(async (value) => {
						if (value) {
							viewSettings.imageSettings.quality = "hd";
						} else {
							viewSettings.imageSettings.quality = "standard";
						}

						this.plugin.saveSettings();
					});
				});
		}
	}

	generateChatSettings(parentContainer: HTMLElement, modelType: string) {
		const settingType = getSettingType(this.viewType);
		const viewSettings = this.plugin.settings[settingType];

		new Setting(parentContainer)
			.setName("Temperature")
			.setDesc(
				modelType !== GPT4All
					? "Defaults to 1. What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top_p but not both."
					: "Higher temperatures (eg., 1.2) increase randomness, resulting in more imaginative and diverse text. Lower temperatures (eg., 0.5) make the output more focused, predictable, and conservative. A safe range would be around 0.6 - 0.85"
			)
			.addText((text) => {
				text.setValue(`${viewSettings.chatSettings.temperature}`);
				text.inputEl.type = "number";
				text.onChange((change) => {
					viewSettings.chatSettings.temperature = parseFloat(change);
					this.plugin.saveSettings();
				});
			});

		new Setting(parentContainer)
			.setName("Tokens")
			.setDesc("Maximum number of tokens in the response. Higher values allow longer responses. Recommended: 4096-8192 for Gemini models.")
			.addText((text) => {
				text.setValue(`${viewSettings.chatSettings.maxTokens}`);
				text.inputEl.type = "number";
				text.onChange((change) => {
					viewSettings.chatSettings.maxTokens = parseInt(change);
					this.plugin.saveSettings();
				});
			});

		if (modelType === openAI) {
			new Setting(parentContainer)
				.setName("Frequency penalty")
				.setDesc(
					"Defaults to 0. Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim."
				)
				.addText((text) => {
					text.setValue(
						`${viewSettings.chatSettings.openAI?.frequencyPenalty}`
					);
					text.inputEl.type = "number";
					text.onChange((change) => {
						viewSettings.chatSettings.openAI!.frequencyPenalty =
							parseFloat(change);
						this.plugin.saveSettings();
					});
				});

			new Setting(parentContainer)
				.setName("Log probs")
				.setDesc(
					"Defaults to false. Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the content of message."
				)
				.addToggle((value) => {
					value.onChange((change) => {
						viewSettings.chatSettings.openAI!.logProbs = change;
						this.plugin.saveSettings();
					});
				});

			new Setting(parentContainer)
				.setName("Top log probs")
				.setDesc(
					"An integer between 0 and 5 specifying the number of most likely tokens to return at each token position, each with an associated log probability. logprobs must be set to true if this parameter is used."
				)
				.addText((text) => {
					text.setValue(
						`${viewSettings.chatSettings.openAI?.topLogProbs}`
					);
					text.inputEl.type = "number";
					text.onChange((change) => {
						viewSettings.chatSettings.openAI!.topLogProbs =
							parseFloat(change);
						this.plugin.saveSettings();
					});
				});

			new Setting(parentContainer)
				.setName("Presence penalty")
				.setDesc(
					"Defaults to 0. Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics."
				)
				.addText((text) => {
					text.setValue(
						`${viewSettings.chatSettings.openAI?.presencePenalty}`
					);
					text.inputEl.type = "number";
					text.onChange((change) => {
						viewSettings.chatSettings.openAI!.presencePenalty =
							parseFloat(change);
						this.plugin.saveSettings();
					});
				});

			new Setting(parentContainer)
				.setName("Response format")
				.setDesc(
					`An object specifying the format that the model must output. Compatible with GPT-4 turbo and all GPT-3.5 turbo models newer than gpt-3.5-turbo-1106. Setting to { "type": "json_object" } enables JSON mode, which guarantees the message the model generates is valid JSON.`
				)
				.addText((text) => {
					text.setValue(
						`${viewSettings.chatSettings.openAI?.responseFormat}`
					);
					text.onChange((change) => {
						viewSettings.chatSettings.openAI!.responseFormat =
							change;
						this.plugin.saveSettings();
					});
				});

			new Setting(parentContainer)
				.setName("Top p")
				.setDesc(
					"Defaults to 1. An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered."
				)
				.addText((text) => {
					text.setValue(`${viewSettings.chatSettings.openAI?.topP}`);
					text.inputEl.type = "number";
					text.onChange((change) => {
						viewSettings.chatSettings.openAI!.topP =
							parseFloat(change);
						this.plugin.saveSettings();
					});
				});
		}
	}

	generateContextSettings(parentContainer: HTMLElement) {
		const settingType = getSettingType(this.viewType);
		const viewSettings = this.plugin.settings[settingType];
		const contextSettings = viewSettings.contextSettings;

		// Context section header
		const contextHeader = parentContainer.createEl("h3", {
			text: "Context Settings",
		});
		contextHeader.style.marginTop = "1.5em";
		contextHeader.style.marginBottom = "0.5em";

		// Include active file
		new Setting(parentContainer)
			.setName("Include active file")
			.setDesc(
				"Automatically include the content of the currently active file in the context sent to the AI."
			)
			.addToggle((toggle) => {
				toggle
					.setValue(contextSettings.includeActiveFile)
					.onChange(async (value) => {
						contextSettings.includeActiveFile = value;
						await this.plugin.saveSettings();
					});
			});

		// Include selection
		new Setting(parentContainer)
			.setName("Include selected text")
			.setDesc(
				"Automatically include any text selected in the editor in the context sent to the AI."
			)
			.addToggle((toggle) => {
				toggle
					.setValue(contextSettings.includeSelection)
					.onChange(async (value) => {
						contextSettings.includeSelection = value;
						await this.plugin.saveSettings();
					});
			});

		// Context token budget percentage
		new Setting(parentContainer)
			.setName("Context token budget (%)")
			.setDesc(
				"Percentage of max tokens to allocate for context (0-100). The remaining percentage is reserved for the AI's response. For example, 70% means 70% for context and 30% for the response."
			)
			.addText((text) => {
				text.setValue(`${contextSettings.maxContextTokensPercent}`);
				text.inputEl.type = "number";
				text.inputEl.min = "0";
				text.inputEl.max = "100";
				text.onChange((change) => {
					const value = parseInt(change);
					if (!isNaN(value) && value >= 0 && value <= 100) {
						contextSettings.maxContextTokensPercent = value;
						this.plugin.saveSettings();
					}
				});
			});

		// Select files button
		const selectFilesSetting = new Setting(parentContainer)
			.setName("Additional files")
			.setDesc(
				"Select additional files from your vault to include in the context."
			)
			.addButton((button) => {
				button.setButtonText("Select Files").onClick(() => {
					const modal = new FileSelector(
						this.plugin.app,
						this.plugin,
						this.viewType,
						contextSettings.selectedFiles,
						(files) => {
							contextSettings.selectedFiles = files;
							this.plugin.saveSettings();
							this.updateSelectedFilesDisplay(
								parentContainer,
								selectFilesSetting.settingEl
							);
						}
					);
					modal.open();
				});
			});

		// Display selected files
		this.updateSelectedFilesDisplay(parentContainer, selectFilesSetting.settingEl);
	}

	updateSelectedFilesDisplay(
		parentContainer: HTMLElement,
		selectFilesSetting: HTMLElement
	) {
		const settingType = getSettingType(this.viewType);
		const contextSettings = this.plugin.settings[settingType].contextSettings;

		// Remove existing display
		const existingDisplay = parentContainer.querySelector(
			".llm-selected-files-display"
		);
		if (existingDisplay) {
			existingDisplay.remove();
		}

		// Create new display if there are selected files
		if (contextSettings.selectedFiles.length > 0) {
			const displayDiv = document.createElement("div");
			displayDiv.className = "llm-selected-files-display";
			displayDiv.style.marginLeft = "2em";
			displayDiv.style.marginTop = "0.5em";
			displayDiv.style.fontSize = "0.9em";

			const title = displayDiv.createEl("div", {
				text: `Selected files (${contextSettings.selectedFiles.length}):`,
			});
			title.style.fontWeight = "500";
			title.style.marginBottom = "0.25em";

			const fileList = displayDiv.createEl("ul");
			fileList.style.margin = "0";
			fileList.style.paddingLeft = "1.5em";

			for (const filePath of contextSettings.selectedFiles) {
				const fileItem = fileList.createEl("li");
				fileItem.style.marginBottom = "0.25em";

				const fileText = fileItem.createEl("span", {
					text: filePath,
				});
				fileText.style.color = "var(--text-muted)";

				const removeButton = fileItem.createEl("button", {
					text: "Ã—",
				});
				removeButton.style.marginLeft = "0.5em";
				removeButton.style.padding = "0 0.5em";
				removeButton.style.cursor = "pointer";
				removeButton.addEventListener("click", () => {
					contextSettings.selectedFiles =
						contextSettings.selectedFiles.filter(
							(f) => f !== filePath
						);
					this.plugin.saveSettings();
					this.updateSelectedFilesDisplay(
						parentContainer,
						selectFilesSetting
					);
				});
			}

			// Insert after the select files setting
			selectFilesSetting.after(displayDiv);
		}
	}

	generateModerationsSettings(parentContainer: HTMLElement) { }
}
